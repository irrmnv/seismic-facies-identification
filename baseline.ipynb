{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark = True\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import argus\n",
    "from argus.callbacks import MonitorCheckpoint, EarlyStopping, LoggingToFile, ReduceLROnPlateau\n",
    "import albumentations as A\n",
    "from pytorch_toolbelt.inference.tiles import ImageSlicer, CudaTileMerger\n",
    "from pytorch_toolbelt.losses import LovaszLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img = np.load('data_train.npz', allow_pickle=True, mmap_mode='r')['data']\n",
    "train_labels = np.load('labels_train.npz', allow_pickle=True, mmap_mode='r')['labels']\n",
    "test_img = np.load('data_test_1.npz', allow_pickle=True, mmap_mode='r')['data']\n",
    "\n",
    "train_labels -= 1\n",
    "\n",
    "train_img.shape, train_labels.shape, test_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_test_img = np.concatenate([train_img, test_img], axis=2)\n",
    "\n",
    "_min, _max = adjusted_test_img.min(), adjusted_test_img.max()\n",
    "\n",
    "train_img = (train_img - _min) / (_max - _min)\n",
    "test_img = (test_img - _min) / (_max - _min)\n",
    "adjusted_test_img = (adjusted_test_img - _min) / (_max - _min)\n",
    "\n",
    "del adjusted_test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verticals = [0 for _ in range(train_img.shape[1])] + [1 for _ in range(train_img.shape[2])]\n",
    "folds = list(StratifiedKFold(n_splits=5, random_state=42, shuffle=True).split(X=verticals, y=verticals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LovaszBCELoss(torch.nn.Module):\n",
    "    def __init__(self, lovasz_weight=0.75, ce_weight=0.25):\n",
    "        super().__init__()\n",
    "        self.lovasz_weight = lovasz_weight\n",
    "        self.ce_weight = ce_weight\n",
    "        self.ce = torch.nn.CrossEntropyLoss()\n",
    "        self.lovasz = LovaszLoss()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        if self.lovasz_weight > 0:\n",
    "            lovasz = self.lovasz(torch.softmax(output, dim=1), target) * self.lovasz_weight\n",
    "        else:\n",
    "            lovasz = 0\n",
    "\n",
    "        if self.ce_weight > 0:\n",
    "            ce = self.ce(output, target.long()) * self.ce_weight\n",
    "        else:\n",
    "            ce = 0\n",
    "\n",
    "        return lovasz + ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeismicFaciesDataset(Dataset):\n",
    "    def __init__(self, img, labels, train=True):\n",
    "        self.img = img\n",
    "        self.labels = labels\n",
    "        self.xaxis = self.img.shape[1]\n",
    "        self.yaxis = self.img.shape[2]\n",
    "\n",
    "        self.aug = A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(p=0.7, shift_limit=0, scale_limit=0.15, rotate_limit=15),\n",
    "            A.RandomCrop(p=1, height=896, width=256),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.xaxis + self.yaxis\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < self.xaxis:\n",
    "            image, mask = self.img[:, idx], self.labels[:, idx]\n",
    "        else:\n",
    "            image, mask = self.img[:, :, idx-self.xaxis], self.labels[:, :, idx-self.xaxis]\n",
    "\n",
    "        image = image[:, :, None]\n",
    "\n",
    "        augmented = self.aug(image=image, mask=mask)\n",
    "        image, mask = augmented['image'], augmented['mask']\n",
    "\n",
    "        return image.transpose(2, 0, 1), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeismicFaciesModel(argus.Model):\n",
    "    nn_module = smp.Unet\n",
    "    optimizer = optim.SGD\n",
    "    loss = LovaszBCELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'nn_module': {\n",
    "        'encoder_name': 'efficientnet-b3',\n",
    "        'decoder_attention_type': 'scse',\n",
    "        'classes': 6,\n",
    "        'in_channels': 1,\n",
    "        'activation': None\n",
    "    },\n",
    "    'loss': {\n",
    "        'lovasz_weight': 0.75,\n",
    "        'ce_weight': 0.25,\n",
    "    },\n",
    "    'optimizer': {'lr': 0.01, 'momentum': 0.9, 'weight_decay': 0.0001},\n",
    "    'device': 'cuda'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(dataset, batch_size, train_index, test_index):\n",
    "    train_dataset, test_dataset = Subset(dataset, train_index), Subset(copy(dataset), test_index)\n",
    "    test_dataset.dataset.aug = A.PadIfNeeded(p=1, min_height=1024, min_width=800)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=16)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SeismicFaciesDataset(train_img, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_index, test_index) in enumerate(folds):\n",
    "    model = SeismicFaciesModel(params)\n",
    "    model.set_device((0, 1))\n",
    "\n",
    "    train_loader, val_loader = get_data_loaders(dataset, batch_size=16, train_index=train_index, test_index=test_index)\n",
    "\n",
    "    callbacks = [\n",
    "        MonitorCheckpoint(dir_path=f'unet_fold_{i}', monitor='val_loss', max_saves=3),\n",
    "        ReduceLROnPlateau(monitor='val_loss', patience=30, factor=0.64, min_lr=1e-8),\n",
    "        EarlyStopping(monitor='val_loss', patience=50),\n",
    "        LoggingToFile(f'unet_fold_{i}.log'),\n",
    "    ]\n",
    "\n",
    "    model.fit(train_loader,\n",
    "          val_loader=val_loader,\n",
    "          num_epochs=700,\n",
    "          metrics=['loss'],\n",
    "          callbacks=callbacks,\n",
    "          metrics_on_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = argus.load_model('unet_fold_0/model-692-0.053404.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiler = ImageSlicer(train_img.shape[:-1] + (1,), tile_size=(896, 256), tile_step=(1, 8))\n",
    "merger = CudaTileMerger(tiler.target_shape, 6, tiler.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = []\n",
    "for img in test_img.transpose(2, 0, 1):\n",
    "    tiles = [tile for tile in tiler.split(img[:, :, None])]\n",
    "\n",
    "    for tiles_batch, coords_batch in DataLoader(list(zip(tiles, tiler.crops)), batch_size=96):\n",
    "        tiles_batch = tiles_batch.permute(0, 3, 1, 2)\n",
    "        pred_batch = torch.softmax(model.predict(tiles_batch), axis=1)\n",
    "        merger.integrate_batch(pred_batch, coords_batch)\n",
    "\n",
    "    merged_mask = merger.merge()\n",
    "    merged_mask = merged_mask.permute(1, 2, 0).cpu().numpy()\n",
    "    merged_mask = tiler.crop_to_orignal_size(merged_mask).argmax(2)\n",
    "    \n",
    "    test_labels.append(merged_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = np.stack(test_labels).transpose(1, 2, 0)\n",
    "\n",
    "np.savez_compressed(\n",
    "    'prediction.npz',\n",
    "    prediction=test_labels.astype(train_labels.dtype) + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}